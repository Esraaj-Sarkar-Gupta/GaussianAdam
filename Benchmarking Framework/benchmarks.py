#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
benchmarks.py

Core benchmark logic for FM216 optimizer project:
1. Well-conditioned quadratic
2. Ill-conditioned quadratic
3. Rosenbrock

Also exposes `my_optimizer`, which wraps the optimizers made by `optimizers.make_optimizer`.
"""

import numpy as np

from optimizers import make_optimizer

# ---- Generic Optimizer Wrapper ---- #

def my_optimizer(
    x0,
    grad_fn,
    f_fn,
    max_iters: int = 1000,
    tol: float = 1e-6,           # gradient-norm tolerance
    f_tol: float | None = 1e-6,  # function-value tolerance
    algo: str = "adam",
    **opt_kwargs,
):
    """
    Generic optimizer wrapper that uses the optimizers defined in `optimizers.py`.

    Stops when:
      -> ||grad||_2 <= tol       (gradient criterion), OR
      -> f(x) <= f_tol           (if f_tol is not None)
    """
    x = np.asarray(x0, dtype=float)

    # IMPORTANT: forward kwargs to the optimizer constructor
    opt = make_optimizer(algo, x.copy(), **opt_kwargs)

    trace = []

    for k in range(max_iters):
        f_val = f_fn(x)
        trace.append(f_val)

        g = grad_fn(x)
        grad_norm = np.linalg.norm(g)

        grad_ok = (grad_norm <= tol)
        f_ok = (f_tol is not None and f_val <= f_tol)

        if grad_ok or f_ok:
            break

        x = opt.step(g)

    return {
        "x": x,
        "f": f_fn(x),
        "n_iters": len(trace),
        "trace": trace,
        "noise_log": getattr(opt, "noise_log", None),
        "gamma" : getattr(opt, "gamma", None)
    }




def make_spd_matrix(dim: int, cond_number: float, random_state: int = 0):
    """
    Create a symmetric positive definite matrix Q with given condition number.

    cond(Q) = λ_max / λ_min ≈ cond_number.
    Eigenvalues are spaced logarithmically between λ_min and λ_max.
    
    Generated by GPT5.1
    """
    rng = np.random.default_rng(random_state)

    A = rng.normal(size=(dim, dim))
    Q_orth, _ = np.linalg.qr(A)

    lambda_min = 1.0
    lambda_max = cond_number * lambda_min
    eigvals = np.logspace(np.log10(lambda_min),
                          np.log10(lambda_max),
                          num=dim)

    Q = (Q_orth * eigvals) @ Q_orth.T
    return Q


def run_benchmark(
    optimizer,
    x0,
    f_fn,
    grad_fn,
    max_iters: int,
    grad_tol: float = 1e-6,
    f_target: float | None = None,
):
    """
    Generic helper: run optimizer and (optionally) compute iterations
    to reach a target function value.

    Parameters
    ----------
    optimizer : callable
        Function with signature (x0, grad_fn, f_fn, max_iters, tol) -> result_dict.
    x0 : np.ndarray
        Initial parameters.
    f_fn, grad_fn : callables
        Objective and gradient.
    max_iters : int
        Maximum iterations.
    grad_tol : float
        Gradient norm tolerance passed to the optimizer.
    f_target : float or None
        Target objective value to measure "iters_to_target".
        
    Generated by GPT5.1
    """
    result = optimizer(x0, grad_fn, f_fn, max_iters=max_iters, tol=grad_tol)

    iters_to_target = None
    trace = result.get("trace", None)

    if trace is not None and f_target is not None:
        trace_arr = np.array(trace)
        idx = np.where(trace_arr <= f_target)[0]
        if len(idx) > 0:
            iters_to_target = int(idx[0])

    return {
        "final_x": result["x"],
        "final_f": result["f"],
        "n_iters": result["n_iters"],
        "iters_to_target": iters_to_target,
        "trace": trace,
        "noise_log": result.get("noise_log", None),
        "gamma" : result.get("gamma", None),
    }

# ---- Quadratic Benchmarks ---- #
def test_quadratic(
    optimizer,
    dim: int = 10,
    cond_number: float = 10.0,
    random_state: int = 0,
    max_iters: int = 10_000,
    grad_tol: float = 1e-6,
    f_target: float = 1e-6,
):
    """
    Test on f(x) = 0.5 x^T Q x, with given condition number.
    """
    Q = make_spd_matrix(dim=dim, cond_number=cond_number, random_state=random_state)

    def f_fn(x):
        return 0.5 * float(x.T @ Q @ x)

    def grad_fn(x):
        return Q @ x

    rng = np.random.default_rng(random_state)
    x0 = rng.normal(size=(dim,))

    result = run_benchmark(
        optimizer=optimizer,
        x0=x0,
        f_fn=f_fn,
        grad_fn=grad_fn,
        max_iters=max_iters,
        grad_tol=grad_tol,
        f_target=f_target,
    )
    return result


def test_well_conditioned_quadratic(optimizer, **kwargs):
    """
    Wrapper for well-conditioned quadratic with cond(Q) approx = 10.
    """
    default_kwargs = dict(dim=10, cond_number=10.0)
    default_kwargs.update(kwargs)
    return test_quadratic(optimizer, **default_kwargs)


def test_ill_conditioned_quadratic(optimizer, **kwargs):
    """
    Wrapper for ill-conditioned quadratic with cond(Q) >> 1.
    """
    default_kwargs = dict(dim=10, cond_number=200.0)
    default_kwargs.update(kwargs)
    return test_quadratic(optimizer, **default_kwargs)

# The functions above were generated by GPT5.1

# ---- Rosenbrock Function ---- #

def rosenbrock_f(x: np.ndarray):
    """
    Standard 2D Rosenbrock:
    f(x, y) = (1 - x)^2 + 100 (y - x^2)^2
    """
    x1, x2 = x[0], x[1]
    return (1.0 - x1) ** 2 + 100.0 * (x2 - x1 ** 2) ** 2


def rosenbrock_grad(x: np.ndarray):
    """
    Gradient of the Rosenbrock function.
    """
    x1, x2 = x[0], x[1]
    df_dx1 = -2.0 * (1.0 - x1) - 400.0 * x1 * (x2 - x1 ** 2)
    df_dx2 = 200.0 * (x2 - x1 ** 2)
    return np.array([df_dx1, df_dx2])


def test_rosenbrock(
    optimizer,
    x0=np.array([-1.2, 1.0]),
    max_iters: int = 50_000,
    grad_tol: float = 1e-6,
    f_target: float = 1e-4,
):
    """
    Test optimizer on Rosenbrock starting from point (-1.2, 1.0).
    """
    result = run_benchmark(
        optimizer=optimizer,
        x0=x0,
        f_fn=rosenbrock_f,
        grad_fn=rosenbrock_grad,
        max_iters=max_iters,
        grad_tol=grad_tol,
        f_target=f_target,
    )
    return result

# ---- Non-convex Benchmarks ---- #

A_RASTRIGIN = 10.0

def rastrigin_f(x):
    """
    Rastrigin function.

    Global minimum: f(0,...,0) = 0
    Many local minima with f > 0.

    x: 1D numpy array
    
    Generated by GPT5.1
    """
    x = np.asarray(x, dtype=float)
    n = x.size
    return A_RASTRIGIN * n + np.sum(x**2 - A_RASTRIGIN * np.cos(2 * np.pi * x))


def rastrigin_grad(x):
    """
    Gradient of the Rastrigin function.
    """
    x = np.asarray(x, dtype=float)
    return 2.0 * x + 2.0 * np.pi * A_RASTRIGIN * np.sin(2 * np.pi * x)

def test_rastrigin(opt, dim=10, max_iters=3000):
    """
    Non-convex benchmark: Rastrigin.
    """
    rng = np.random.default_rng(0)

    # Start somewhere away from the global min -- [-5, 5]^dim
    x0 = rng.uniform(-10.0, 10.0, size=dim)
    initial_f = rastrigin_f(x0)

    trace = []
    def f_wrapped(x):
        val = rastrigin_f(x)
        trace.append(val)
        return val

    def g_wrapped(x):
        return rastrigin_grad(x)

    res = opt(x0, g_wrapped, f_wrapped, max_iters=max_iters)

    # Pack result in the same style as your other tests
    res_dict = {
        "x_final": res.get("x_final", None) if isinstance(res, dict) else None,
        "final_f": trace[-1] if len(trace) > 0 else None,
        "initial_f" : initial_f,
        "n_iters": len(trace),
        "trace": np.array(trace),
        "noise_log": res.get("noise_log", None) if isinstance(res, dict) else None,
    }
    return res_dict
